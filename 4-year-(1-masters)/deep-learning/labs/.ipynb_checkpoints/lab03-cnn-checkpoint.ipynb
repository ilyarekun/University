{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e855963",
   "metadata": {},
   "source": [
    "# Convolutional Networks in PyTorch\n",
    "\n",
    "In the last exercise, we explained what convolution and pooling are, how these operations work, and what they are used for in the context of neural networks. The goal of this exercise is to train simple neural networks to classify images of handwritten digits. We will use the *PyTorch* framework, which is used for training neural networks. If you haven't installed it yet, [follow the current installation guide](https://pytorch.org/get-started/locally/). Then, download this notebook and work in it locally, or for example, on [Google Colab](https://colab.research.google.com).\n",
    "\n",
    "If you are running the notebook via Colab, use the following code to work with the graphics card. However, since the dataset is simple, if you don't have a GPU setup on your computer, you can still train the network in a few minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc83efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    from os.path import exists\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
    "\n",
    "try: \n",
    "    import torchbearer\n",
    "except:\n",
    "    !pip install torchbearer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc934e7",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Before we prepare the structure of the neural network, we need to load the training and testing data. In this exercise, we will use the standard [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), as training a network on it has become a sort of modern-day version of the *Hello, world!* program. The dataset contains handwritten digits from 0-9 in the form of grayscale images with dimensions of 28×28.\n",
    "\n",
    "![MNIST dataset example](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448afd2",
   "metadata": {},
   "source": [
    "Before loading the data itself, let's first import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reload external modules if they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchbearer\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchbearer import Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132509f",
   "metadata": {},
   "source": [
    "The very first necessary step when loading images is the need to transform them into tensor representation. In PyTorch, each image is represented as a three-dimensional tensor with dimensions `[channels][height][width]`, where `channels` is the number of color channels (3 for RGB images, 1 for grayscale). These transformations are defined as a list or a series of simple operations that will be applied when loading the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df947690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each image to tensor format\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # convert to tensor\n",
    "])\n",
    "\n",
    "# load data\n",
    "trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
    "testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
    "\n",
    "# create data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883b624",
   "metadata": {},
   "source": [
    "In addition to loading the dataset, we will also create dataloaders that will be responsible for delivering the used data in batches. Besides the batch size, we will also define the loading method; here, for example, we set a random order for usage because regular neural networks are trained efficiently when they are trained on temporally independent data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d84aae",
   "metadata": {},
   "source": [
    "## Definition of the Neural Network\n",
    "\n",
    "The next step is to define the model of the neural network. To begin with, we will add only one convolutional layer and padding to our network, so you can see the basic structure of a convolutional block. For completeness of the implementation, we will also add an activation layer as well as `Dropout` to prevent overfitting. The structure of the network is as follows:\n",
    "\n",
    "1. The first hidden layer (the input one will be created automatically) is a convolutional layer `Convolution2D`. The layer contains 32 feature maps with 5×5 kernels and a ReLU activation layer.\n",
    "2. The next layer is a max pooling layer `MaxPooling2D`. The filter size is 2×2.\n",
    "3. Then we use a regularization layer `Dropout`, which randomly zeroes out 20% of the neurons in the layer to avoid overfitting.\n",
    "4. Before reaching the classification part of the neural network, we need to adjust the output of the previous layers so that it can be processed by fully connected layers. This is done by the *flatten* operation, which transforms the tensor representation into a vector that can be processed by traditional layers.\n",
    "5. We proceed with a layer of 128 neurons and another ReLU activation layer.\n",
    "6. The output layer is given by the number of classes, so it contains 10 neurons.\n",
    "\n",
    "To define the network, we will create a subclass of `nn.Module`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21067414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (5, 5), padding=0)\n",
    "        self.fc1 = nn.Linear(32 * 12**2, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, (2,2))\n",
    "        out = F.dropout(out, 0.2)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc34881",
   "metadata": {},
   "source": [
    "When generating vectors using the `view` method, the second parameter is set to `-1`, which serves to automatically calculate the second dimension and ensures that the batch size is preserved. The input to the `forward` method therefore has dimensions `[batch_size][channels][height][width]`, and the output will have dimensions `[batch_size][num_classes=10]`.\n",
    "\n",
    "**What are the dimensions of the inputs and outputs for each layer?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057158c",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "After defining the model, we can train our network. We will use the cross-entropy loss function and the ADAM optimizer. We will train for 5 epochs with a batch size of 128. For a simpler implementation of training and evaluation, we can use `torchbearer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
    "trial.with_generators(trainloader, test_generator=testloader)\n",
    "trial.run(epochs=5)\n",
    "results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fcedb",
   "metadata": {},
   "source": [
    "## Network Extension\n",
    "\n",
    "Create an extended neural network structure with the following architecture:\n",
    "\n",
    "1. Convolutional layer with 30 feature maps of size 5×5 and ReLU activation.\n",
    "2. Max pooling layer with dimensions 2×2.\n",
    "3. Convolutional layer with 15 feature maps of size 3×3 and ReLU activation.\n",
    "4. Max pooling layer with dimensions 2×2.\n",
    "5. Dropout layer with a probability of 20%.\n",
    "6. Flatten layer.\n",
    "7. Fully connected layer with 128 neurons and ReLU activation.\n",
    "8. Fully connected layer with 50 neurons and ReLU activation.\n",
    "9. Output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Model Definition\n",
    "class BetterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BetterCNN, self).__init__()\n",
    "        # TODO: define layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: define structure\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a7760",
   "metadata": {},
   "source": [
    "Use the following code for training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=True)\n",
    "\n",
    "# build the model\n",
    "model = BetterCNN()\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
    "trial.with_generators(trainloader, test_generator=testloader)\n",
    "trial.run(epochs=5)\n",
    "results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4329eff",
   "metadata": {},
   "source": [
    "## Uloženie siete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c189ba2",
   "metadata": {},
   "source": [
    "After training a network, you often need to save the weights and load them for future use. In PyTorch, the method `torch.save(state, filepath)` is used to save the model, which stores the weight values into a file so that they can be loaded later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee229dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model weights\n",
    "torch.save(model.state_dict(), \"./bettercnn.weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038705c4",
   "metadata": {},
   "source": [
    "If you are working in Colab, you can download the model using the method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('bettercnn.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0f2b1",
   "metadata": {},
   "source": [
    "## Using the Network\n",
    "\n",
    "Once you have trained and saved the network, you probably want to use it for making predictions during the model's lifetime, for example, within an application. Ideally, the network should be able to process inputs that were not part of the original dataset but share the same fundamental characteristics. To verify the functionality of your solution from the previous step, download the [sample digit images](lab03/imgs.zip).\n",
    "\n",
    "Next, load the saved weights. **Note:** By default, PyTorch only saves the weights, not the network structure. Therefore, if you're writing code across multiple files, you will often need to copy/import the class defining the network into the file where you are handling the weight loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# build the model and load state\n",
    "model = BetterCNN()\n",
    "model.load_state_dict(torch.load('bettercnn.weights'))\n",
    "\n",
    "# put model in eval mode\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58310036",
   "metadata": {},
   "source": [
    "On the last line of the previous code block, we enable the so-called *eval* mode, which sets the network for inference use. In this mode, the weight values are not updated, and some layers, such as `Dropout` or `BatchNorm2D`, are disabled.\n",
    "\n",
    "Next, we load and visualize a sample input (you can choose a different sample as well):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ad391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "im = transform(Image.open(\"imgs/1.png\"))\n",
    "\n",
    "plt.imshow(im[0], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931487c",
   "metadata": {},
   "source": [
    "Now we can use the model for prediction, but the network expects a batch as input, while we are providing just one image. That's why we call the `unsqueeze(0)` method, which adds the necessary dimensions. The network's output will then be 10 values, each representing the prediction score for the individual classes. The final prediction of the network is the class with the highest predicted value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = im.unsqueeze(0)\n",
    "predictions = model(batch)\n",
    "\n",
    "print(\"logits:\", predictions.data)\n",
    "\n",
    "_, predicted_class = predictions.max(1)\n",
    "\n",
    "print(\"predicted class:\", predicted_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440306f9",
   "metadata": {},
   "source": [
    "**Check the network's prediction on all the sample data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6526b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "for i in range(10):\n",
    "    im = transform(Image.open(\"imgs/{}.png\".format(i)))\n",
    "\n",
    "    plt.imshow(im[0], cmap=plt.get_cmap('gray'))\n",
    "    batch = im.unsqueeze(0)\n",
    "    predictions = model(batch)\n",
    "\n",
    "    print(\"logits:\", predictions.data)\n",
    "\n",
    "    _, predicted_class = predictions.max(1)\n",
    "\n",
    "    print(\"predicted class:\", predicted_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09cedb",
   "metadata": {},
   "source": [
    "## Visualization of Filters\n",
    "\n",
    "To gain an intuitive understanding of how a neural network works, we can visualize various characteristics of individual layers and filters. Filters can be visualized because they can be considered as small images that describe the basic features of the inputs. The filter weights can be loaded directly from the trained network and visualized using `matplotlib`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.conv1.weight.data.cpu()\n",
    "\n",
    "# plot the first layer features\n",
    "for i in range(0,30):\n",
    "    plt.subplot(5,6,i+1)\n",
    "    plt.imshow(weights[i, 0, :, :], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2864471",
   "metadata": {},
   "source": [
    "The value `model.conv1.data` is a tensor containing the weight values, and the `cpu()` method loads the data from the GPU to the processor for better visualization. Since the influence of individual weights is aggregated as we move through the network, it often makes sense to visualize only the filters of the first layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd857836",
   "metadata": {},
   "source": [
    "## Visualization of Feature Maps\n",
    "\n",
    "In a similar way to the filters themselves, we can visualize their effect on a selected input by passing the input through the network and obtaining the outputs at any layer. In PyTorch, this can be done using a `hook` object, which is used to pause the forward pass at a certain layer. For example, for the second convolutional layer, we can use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "im = transform(Image.open(\"imgs/1.PNG\")).unsqueeze(0)\n",
    "\n",
    "def hook_function(module, grad_in, grad_out):\n",
    "    for i in range(grad_out.shape[1]):\n",
    "        conv_output = grad_out.data[0, i]\n",
    "        plt.subplot(5, int(1+grad_out.shape[1]/5), i+1)\n",
    "        plt.imshow(conv_output, cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "hook = model.conv2.register_forward_hook(hook_function) # register the hook\n",
    "model(im) # forward pass\n",
    "hook.remove() #Tidy up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569250f",
   "metadata": {},
   "source": [
    "**Try visualizing the outputs of the first convolutional layer.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd438e",
   "metadata": {},
   "source": [
    "## Visualization of Maximum Activation\n",
    "\n",
    "The last useful way to visualize what the filters have learned is to find an input image that would cause the maximum activation of a filter. We can obtain such an image by generating random noise, which we optimize using gradient ascent until we find the maximization for the given filter. The following code will generate exactly such an image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_maximum_activation(model, target, num=10, alpha = 1.0):\n",
    "    for selected in range(num):\n",
    "        input_img = torch.randn(1, 1, 28, 28, requires_grad=True)\n",
    "\n",
    "        # we're interested in maximising outputs of the 3rd layer:\n",
    "        conv_output = None\n",
    "\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            nonlocal conv_output\n",
    "            # Gets the conv output of the selected filter/feature (from selected layer)\n",
    "            conv_output = grad_out[0, selected]\n",
    "\n",
    "        hook = target.register_forward_hook(hook_function)\n",
    "\n",
    "        for i in range(30):\n",
    "            model(input_img)\n",
    "            loss = torch.mean(conv_output)\n",
    "            loss.backward()\n",
    "\n",
    "            norm = input_img.grad.std() + 1e-5\n",
    "            input_img.grad /= norm\n",
    "            input_img.data = input_img + alpha * input_img.grad\n",
    "\n",
    "        hook.remove()\n",
    "\n",
    "        input_img = input_img.detach()\n",
    "\n",
    "        plt.subplot(2,num/2,selected+1)\n",
    "        plt.imshow(input_img[0,0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "visualise_maximum_activation(model, model.fc3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
