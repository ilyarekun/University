{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb320d06",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) (without Deep Learning for now)\n",
    "\n",
    "A large amount of textual data can serve as a source of valuable information, and for this reason, **natural language processing** (NLP) has become an important subfield of machine learning in recent years. In the upcoming exercises, we will explore the basic principles of NLP and tasks specific to texts. In today's exercise, we will demonstrate an example of **sentiment analysis**, which involves categorizing text based on the author's perspective or opinion.\n",
    "\n",
    "For now, we will not use neural networks for this task, as it is important for you to understand the sequence of steps involved in text processing. In the next exercise, we will show how a special type of neural network, a recurrent neural network (RNN), can help with this task. We will conclude our exploration of NLP by looking at **attention mechanisms** and **transformers**, which represent the state-of-the-art in NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f95f70",
   "metadata": {},
   "source": [
    "In today's exercise, we will work with the **IMDb dataset**, which contains 50,000 movie reviews categorized into two classes: **positive** (more than 6 stars) and **negative** (less than 5 stars). After downloading the dataset, we will tackle tasks such as:\n",
    "\n",
    "1. **Data preprocessing**;\n",
    "2. **Text data vectorization**;\n",
    "3. **Training a machine learning model for classification**;\n",
    "4. **Working with large text data**;\n",
    "5. **Estimating the content of the text**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce4b91",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "[You can download the dataset from this link](http://ai.stanford.edu/~amaas/data/sentiment/), then unzip the file (approximately 80 MB).\n",
    "\n",
    "You might notice that the data is split into two directories for training and testing, and within these folders, you will find many individual files. For easier handling, we will combine these files into a single CSV file (this process might take a few minutes). If you prefer not to wait for the results, [you can download the pre-processed CSV file here](lab07/movie_data.csv) (approximately 64 MB).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cefda31",
   "metadata": {},
   "source": [
    "You can unzip the files directly in Python, which might be faster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43801724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(\"lab07/aclImdb_v1.tar.gz\", 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "# pip install pyprind\n",
    "import pyprind\n",
    "\n",
    "BASEPATH = \"lab07/aclImdb\"\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "df = pd.DataFrame()\n",
    "for subdir in ('test', 'train'):\n",
    "    for cat in ('pos', 'neg'):\n",
    "        path = os.path.join(BASEPATH, subdir, cat)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[cat]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17e528",
   "metadata": {},
   "source": [
    "Before saving the loaded data, we will shuffle it randomly and then save it to a CSV file (if you downloaded the pre-made file, you can skip these steps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(\"movie_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97f409",
   "metadata": {},
   "source": [
    "Next, we'll load the dataset again (this step is necessary), make some adjustments, and check the amount and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2429fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_data.csv\", encoding='utf-8')\n",
    "# krok potrebny na niektorych pocitacoch\n",
    "# df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb3463",
   "metadata": {},
   "source": [
    "## 2. Vectorization of Text Data\n",
    "\n",
    "Neural networks, as well as other machine learning algorithms, were designed to work with numerical data, which is not the case for text. Therefore, for text and other categorical data, it is necessary to transform this data into a numerical representation. In this step, we will use the **bag-of-words** approach, which assigns a unique feature vector to each word. This process will take place in two steps:\n",
    "\n",
    "1. We will create a collection of unique tokens—such as words—from all the documents.\n",
    "2. We will construct a feature vector for each document, where the vector contains information about how many times a particular word appears in that document.\n",
    "\n",
    "It is clear that most of the values in the vectors will be 0, i.e., the vectors will be **sparse**, which is exactly what we need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4d119",
   "metadata": {},
   "source": [
    "### 2.1. Generating Feature Vectors\n",
    "\n",
    "To generate feature vectors, we will use the `scikit-learn` library, which is part of the Anaconda installation. We will demonstrate the process using simple data and later apply it to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['Roses are red',\n",
    "                 'Violets are blue',\n",
    "                 'Roses are read, violets are blue, wine costs less than dinner for two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337e0e3",
   "metadata": {},
   "source": [
    "Next, we can check and analyze the contents of the generated feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d34e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71256e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbff54",
   "metadata": {},
   "source": [
    "The generated vocabulary represents the index of each word in the vector representation, and the vectors contain the frequency count of each word in the sentence. This count is also referred to as the **raw term frequencies**: *tf(t, d)*, which denotes the number of occurrences of term *t* in document *d*. The order of these terms does not matter, as it is derived from the indices of the vocabulary (usually sorted alphabetically).\n",
    "\n",
    "**Note**: In our bag-of-words model, we used a 1-gram (unigram) model, but there are other representations where a single term consists of multiple tokens, such as bigrams: *roses are*, *are red*. Different tasks require different types of representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b1800",
   "metadata": {},
   "source": [
    "A disadvantage of the TF representation is that some words frequently appear in examples of both types (positive and negative), and therefore they typically don't provide significant value for classification. Instead of considering their raw frequency in the data, we can use the **term frequency-inverse document frequency** technique:\n",
    "\n",
    "$$\n",
    "tf{\\text -}idf(t, d) = tf(t, d) \\times idf(t, d),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "idf(t,d) = \\log \\frac{n_{d}}{1 + df(d, t)}.\n",
    "$$\n",
    "\n",
    "Here, $n_{d}$ is the total number of documents, and $df(d, t)$ is the number of documents *d* containing term *t*. Specific implementations in the `scikit-learn` library work with minor adjustments, but that's beyond the scope for now.\n",
    "\n",
    "We can convert our representation into the TF-IDF form using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b263acf",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning\n",
    "\n",
    "Real-world text data often contain special characters that don't carry any meaningful information, and therefore, it is advisable to remove them. Take the example from a movie review:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fde44",
   "metadata": {},
   "source": [
    "The text contains HTML markup as well as punctuation marks. While punctuation can be crucial in certain cases for text evaluation, in our case, it is unnecessary. Therefore, we will remove both punctuation and HTML tags. Additionally, we will process emojis as a bonus.\n",
    "\n",
    "Here’s an example of text before and after cleaning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessor(df.loc[0, 'review'][-50:]))\n",
    "print(preprocessor(\"</a>This :) is :( a test :-)!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac60e5",
   "metadata": {},
   "source": [
    "Now, we can apply the preprocessing steps to our actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d498089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514469e1",
   "metadata": {},
   "source": [
    "### 2.3. Tokenization of Documents\n",
    "\n",
    "The first step in processing text is splitting it into smaller chunks, called tokens, which are typically words. The basic approach for splitting text into words is to divide sentences based on spaces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "print(tokenizer(\"runners like running and thus they run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379a296",
   "metadata": {},
   "source": [
    "### 2.4. Stemming (and Lemmatization)\n",
    "\n",
    "As you can see in the previous example, some words (such as *running* and *run*, and partially *runners*) represent similar concepts. Therefore, it is unnecessary to have them multiple times in the vocabulary. We also need to process plural forms and convert words into their singular form. This process is called **stemming**, and there are several algorithms to perform it. One of the most basic ones is **Porter stemming**, which is available in the Natural Language Toolkit (NLTK) library, a part of Anaconda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer_porter(\"runners like running and thus they run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc975b6",
   "metadata": {},
   "source": [
    "As shown in the example above, stemming has its limitations, as it is not always perfect (for example, *thus* was incorrectly reduced to *thu*). The second similar process, **lemmatization**, always returns the base dictionary form of a word. While lemmatization is computationally more expensive, it typically produces better results.\n",
    "\n",
    "#### Key Differences:\n",
    "- **Stemming** is faster but may result in incorrect reductions (e.g., *thus* → *thu*).\n",
    "- **Lemmatization** is slower but more accurate, as it returns the correct base form of the word based on its meaning and context.\n",
    "\n",
    "In many NLP tasks, you can choose the method based on your requirements: if speed is more important and minor inaccuracies are acceptable, use stemming. If precision is critical, lemmatization is the better choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17042098",
   "metadata": {},
   "source": [
    "### 2.5. Removal of Stop Words\n",
    "\n",
    "Stop words are words that occur frequently in all texts and therefore do not carry much meaningful information for processing, as they have little informational value for classification or other machine learning tasks. Although *tf-idf* partially eliminates the need for removing such words by reducing the importance of frequently occurring words, in some cases, it is important to remove these words. There are predefined sets available for different languages to accomplish this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86481cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07618b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print([w for w in tokenizer_porter(\"a runner likes running and runs a lot\") if w not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac45e7",
   "metadata": {},
   "source": [
    "## 3. Training a Classification Model\n",
    "\n",
    "In today's exercise, we will not be using neural networks, but we will demonstrate the process of training a classification model using logistic regression. During the training, we will also use hyperparameter optimization with `sklearn`. First, we will prepare the training and test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb33d01",
   "metadata": {},
   "source": [
    "Next, we will train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "small_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    }\n",
    "]\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy', cv=5,\n",
    "                           verbose=2, n_jobs=1)  # n_jobs=-1 for parallel processing\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952191d6",
   "metadata": {},
   "source": [
    "The most successful parameter settings and the corresponding results can be found using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bed585",
   "metadata": {},
   "source": [
    "We can also verify this on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69196c3f",
   "metadata": {},
   "source": [
    "## 4. Working with Large Text Datasets\n",
    "\n",
    "In the previous step, you may have noticed that hyperparameter optimization can take quite a long time when preprocessing data, such as tokenization and subsequent stemming. If you have a large dataset, it is possible that the entire dataset may not even fit into the computer's memory, which can cause such a search to fail.\n",
    "\n",
    "Similar problems with neural networks are solved by minibatch training, and a similar approach exists for other models, called **out-of-core learning**, where the model is trained only partially on a smaller number of examples using the `partial_fit` function.\n",
    "\n",
    "In the first step, we define a new tokenizer with stop word removal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baab296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def new_tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9c86f",
   "metadata": {},
   "source": [
    "Next, we define a generator that will return individual documents from the entire dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(stream_docs(path=\"movie_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca2eb4",
   "metadata": {},
   "source": [
    "The next function will provide a single minibatch of training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b978605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889472a3",
   "metadata": {},
   "source": [
    "When vectorizing, we can no longer use `CountVectorizer` or `TfidfVectorizer`, as these methods require information about the total number of word occurrences in the dataset. Instead, we will use a different type of vectorization, specifically `HashingVectorizer`, which is data-independent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0319bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,  # vector size - estimate\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=new_tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path=\"movie_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d920375",
   "metadata": {},
   "source": [
    "Finally, we can begin the training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d827e",
   "metadata": {},
   "source": [
    "After successful training, we can also check the training accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b22a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbb39a",
   "metadata": {},
   "source": [
    "The final accuracy may be slightly lower than in the previous case, but out-of-core training is much faster and less memory-intensive.\n",
    "\n",
    "Since we used the \"test\" set for validation, if we are satisfied with the results, we can also use this data for training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192c056",
   "metadata": {},
   "source": [
    "## 5. Estimating Text Content\n",
    "\n",
    "Estimating text content can help us in clustering unlabeled text data, where we try to assign labels to individual texts based on their content. One possible algorithm for solving this type of problem is **Latent Dirichlet Allocation**, or LDA, which is based on Bayesian inference. Its goal is to find words that frequently occur together across multiple documents, thus defining a topic or category. Its input is a bag-of-words model, from which it generates two matrices: document-to-topic and word-to-topic. By multiplying them, we can recover the original text with minimal error. A hyperparameter of LDA is the number of topics to be found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f855c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\", encoding='utf-8')\n",
    "# krok potrebny na niektorych pocitacoch\n",
    "# df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b89d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,  # ignore words with a frequency above 10%\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561e28e",
   "metadata": {},
   "source": [
    "After training, we can visualize the most frequent words in each category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                    [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cbece",
   "metadata": {},
   "source": [
    "Sample examples for the *horror* category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c2542",
   "metadata": {},
   "source": [
    "## Sources Used\n",
    "\n",
    "* **IMDb dataset**: Maas, Andrew, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. \"Learning word vectors for sentiment analysis.\" In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 142-150. 2011.\n",
    "* **Porter stemmer:** Porter, Martin F. \"An algorithm for suffix stripping.\" Program 14, no. 3 (1980): 130-137.\n",
    "* **Natural Language Toolkit:** https://www.nltk.org\n",
    "* **Latent Dirichlet allocation:** Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3, no. Jan (2003): 993-1022.\n",
    "* Raschka, Sebastian, Yuxi Hayden Liu, Vahid Mirjalili, and Dmytro Dzhulgakov. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
