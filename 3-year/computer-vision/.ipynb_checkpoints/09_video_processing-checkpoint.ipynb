{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t6-woW77HRBJ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Computer vision - Week_10 - Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T14:01:37.728363Z",
     "start_time": "2023-11-27T14:01:37.692219Z"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1732610855983,
     "user": {
      "displayName": "ILYA REKUN",
      "userId": "11229790335325589174"
     },
     "user_tz": -60
    },
    "id": "h1CotWfuHRBK"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'area_bbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regionprops\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mrc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDejaVu Sans\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont.size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/17.ComputerVision25/venv/lib64/python3.13/site-packages/lazy_loader/__init__.py:82\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[1;32m     81\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 82\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.13/importlib/__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/17.ComputerVision25/venv/lib64/python3.13/site-packages/skimage/measure/_regionprops.py:1426\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(RegionProperties, p)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m prop_doc[p]\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __debug__:\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m# don't install docstrings when in optimized/non-debug mode\u001b[39;00m\n\u001b[0;32m-> 1426\u001b[0m     \u001b[43m_install_properties_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/17.ComputerVision25/venv/lib64/python3.13/site-packages/skimage/measure/_regionprops.py:1421\u001b[0m, in \u001b[0;36m_install_properties_docs\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1418\u001b[0m prop_doc \u001b[38;5;241m=\u001b[39m _parse_docs()\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [member \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(RegionProperties) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m member\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)]:\n\u001b[0;32m-> 1421\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(RegionProperties, p)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mprop_doc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'area_bbox'"
     ]
    }
   ],
   "source": [
    "import time, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage import draw\n",
    "from IPython import display\n",
    "from skimage import morphology\n",
    "from IPython.display import Video\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import cv2\n",
    "\n",
    "from skimage.measure import label\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "plt.rc('font', **{'family' : 'DejaVu Sans', 'weight' : 'normal'})\n",
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6mD5c8prHRBL",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## How to work with video\n",
    "\n",
    "`cv2.VideoCapture` is a class in the OpenCV library that provides a convenient interface for capturing video streams, either from a video file or a live camera feed. It is commonly used in computer vision and image processing applications for tasks such as video analysis, object tracking, and real-time image processing.\n",
    "\n",
    "**Basic methods**:\n",
    "\n",
    "1. `read()`:\n",
    "The `read()` method reads the next frame from the video source. It returns a boolean value (`True` if a frame is successfully read, `False` otherwise) and the frame itself.\n",
    "Example use case: `ret, frame = cap.read()`\n",
    "2. `get(propId)`:\n",
    "The `get()` method is used to retrieve various properties of the video capture, such as frame width, frame height, frames per second (fps), etc. It takes a property identifier (`propId`) as an argument, and returns the corresponding property value.\n",
    "Example use case: `frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))`\n",
    "3. `set(propId, value)`:\n",
    "The `set()` method is used to set the value of a specific property. It takes a property identifier (`propId`) and the desired value.\n",
    "Example use case: `cap.set(cv2.CAP_PROP_POS_FRAMES, 30)`\n",
    "4. `release()`:\n",
    "The `release()` method is used to release the video capture object and close the associated video source (file or camera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:36:18.047407Z",
     "start_time": "2023-11-26T14:36:17.969380Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1732610856364,
     "user": {
      "displayName": "ILYA REKUN",
      "userId": "11229790335325589174"
     },
     "user_tz": -60
    },
    "id": "n55I1m_BHRBM",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment and use one of the following lines to choose the video source:\n",
    "\n",
    "# Use a pre-recorded video file (replace \"example files/traffic.mp4\" with the path to your video file)\n",
    "cap = cv2.VideoCapture(\"example files/traffic.mp4\")\n",
    "\n",
    "# Use the built-in camera (0 represents the default camera)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# If you're a Mac user with an iPhone, you might want to use the second camera (1 represents the second camera)\n",
    "# cap = cv2.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1MDLmnMsHRBN",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Get video information\n",
    "\n",
    "Experiment with possible information you can acquire from `VideoCapture`:\n",
    "\n",
    "`cv2.CAP_PROP_POS_MSEC`: Current position of the video file in milliseconds.\n",
    "`cv2.CAP_PROP_POS_FRAMES`: Index of the next frame to be captured.\n",
    "`cv2.CAP_PROP_POS_AVI_RATIO`: Relative position of the video file (0 for the start, 1 for the end).\n",
    "`cv2.CAP_PROP_FRAME_WIDTH`: Width of the frames in the video.\n",
    "`cv2.CAP_PROP_FRAME_HEIGHT`: Height of the frames in the video.\n",
    "`cv2.CAP_PROP_FPS`: Frames per second in the video.\n",
    "`cv2.CAP_PROP_FOURCC`: 4-character code of codec.\n",
    "`cv2.CAP_PROP_FRAME_COUNT`: Total number of frames in the video.\n",
    "`cv2.CAP_PROP_BRIGHTNESS`: Brightness of the image (only for cameras).\n",
    "`cv2.CAP_PROP_CONTRAST`: Contrast of the image (only for cameras).\n",
    "`cv2.CAP_PROP_SATURATION`: Saturation of the image (only for cameras).\n",
    "`cv2.CAP_PROP_HUE`: Hue of the image (only for cameras).\n",
    "`cv2.CAP_PROP_GAIN`: Gain of the image (only for cameras).\n",
    "`cv2.CAP_PROP_EXPOSURE`: Exposure of the image (only for cameras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:36:18.492192Z",
     "start_time": "2023-11-26T14:36:18.486548Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732610856365,
     "user": {
      "displayName": "ILYA REKUN",
      "userId": "11229790335325589174"
     },
     "user_tz": -60
    },
    "id": "TS52nDBAHRBN",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4d6d1476-e7e2-4687-b96f-bc466e5d8251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame width: 0\n",
      "Frame height: 0\n",
      "Length (frames count): 0\n",
      "Frames per second: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the frame width of the video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# Get the frame height of the video\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Get the total number of frames in the video\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Get the frames per second (fps) of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Print the obtained video properties\n",
    "print(\"Frame width:\", frame_width)\n",
    "print(\"Frame height:\", frame_height)\n",
    "print(\"Length (frames count):\", length)\n",
    "print(\"Frames per second:\", fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "R66XDtKvHRBO",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Video preview\n",
    "#### In jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732610856365,
     "user": {
      "displayName": "ILYA REKUN",
      "userId": "11229790335325589174"
     },
     "user_tz": -60
    },
    "id": "sHNTYEcCHRBO",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # If the frame is not read successfully, break the loop\n",
    "    if not ret:\n",
    "        break\n",
    "    plt.imshow(frame)\n",
    "    plt.draw()\n",
    "    display.clear_output(wait= True)\n",
    "    display.display(plt.gcf())\n",
    "    # time.sleep(.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GciqK7iOHRBO",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### In OS window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:21.579064Z",
     "start_time": "2023-11-26T14:37:20.902901Z"
    },
    "collapsed": false,
    "id": "Xq4pvpGtHRBP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get the frames per second (fps) of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Create a window to display the frames\n",
    "cv2.namedWindow('Video Preview', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame is not read successfully, break the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Video Preview', frame)\n",
    "\n",
    "    # Exit if the 'Esc' key is pressed\n",
    "    if cv2.waitKey(int(1000 / fps)) & 0xFF == 27:\n",
    "        # cap.release()\n",
    "        # cv2.destroyAllWSindows()\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the window\n",
    "# cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:21.591492Z",
     "start_time": "2023-11-26T14:37:21.584084Z"
    },
    "collapsed": false,
    "id": "jb1MTotYHRBP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Clear Video capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zQ7VOBsoHRBP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Support function for both types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KZSE8kOHRBP"
   },
   "source": [
    "In the following code, we extend the video by adding a text to the frame showing the frame number. You can select whether you want to display the preview in jupyter or a system window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:21.672965Z",
     "start_time": "2023-11-26T14:37:21.606309Z"
    },
    "collapsed": false,
    "id": "8oqvjd-lHRBP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add text overlay into video frame\n",
    "def add_text_to_frame(frame, text, position=(30, 30), font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.3, color=(0, 255, 0), thickness=1):\n",
    "    \"\"\"\n",
    "    Add text to a frame.\n",
    "\n",
    "    Parameters:\n",
    "    - frame (numpy.ndarray): Input frame.\n",
    "    - text (str): Text to be added to the frame.\n",
    "    - position (tuple): Position of the text (x, y).\n",
    "    - font (int): Font type.\n",
    "    - font_scale (float): Font scale.\n",
    "    - color (tuple): Text color (B, G, R).\n",
    "    - thickness (int): Text thickness.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Frame with added text.\n",
    "    \"\"\"\n",
    "    frame_with_text = frame.copy()\n",
    "    cv2.putText(frame_with_text, text, position, font, font_scale, color, thickness)\n",
    "    return frame_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:21.673773Z",
     "start_time": "2023-11-26T14:37:21.624529Z"
    },
    "collapsed": false,
    "id": "ZRR-HYR1HRBQ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preview_video(capture_path, env='window', info=True):\n",
    "    \"\"\"\n",
    "    Preview a video from the specified capture path.\n",
    "\n",
    "    Parameters:\n",
    "    - capture_path (str): Path to the video file.\n",
    "    - env (str): Environment for preview. Possible values: 'window' or 'jupyter'.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Open the video capture object\n",
    "    cap = cv2.VideoCapture(capture_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    print(\"Frame width:\", frame_width)\n",
    "    print(\"Frame height:\", frame_height)\n",
    "    print(\"Length (frames count):\", length)\n",
    "    print(\"Frames per second:\", fps)\n",
    "\n",
    "    # Check if the video capture object is successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the frames per second (fps) of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    frame_index = 0  # Variable to keep track of the current frame index\n",
    "\n",
    "    if env == 'window':\n",
    "        # Create a window to display the frames\n",
    "        cv2.namedWindow('Video Preview', cv2.WINDOW_NORMAL)\n",
    "\n",
    "        while True:\n",
    "            # Read a frame from the video\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame is not read successfully, break the loop\n",
    "            if not ret:\n",
    "                break\n",
    "            if info:\n",
    "                frame = add_text_to_frame(frame, \"FRAME: {}\".format(frame_index), position=(5,10))\n",
    "\n",
    "            # Display the frame\n",
    "            cv2.imshow('Video Preview', frame)\n",
    "\n",
    "\n",
    "            # Increment the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "            # Exit if the 'Esc' key is pressed\n",
    "            if cv2.waitKey(int(1000 / fps)) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "        # Release the video capture object and close the window\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    elif env == 'jupyter':\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame is not read successfully, break the loop\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Display the frame index\n",
    "            # print(\"Frame Index:\", frame_index)\n",
    "            if info:\n",
    "                frame = add_text_to_frame(frame, \"FRAME: {}\".format(frame_index), position=(5,10))\n",
    "            plt.imshow(frame)\n",
    "            plt.draw()\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            time.sleep(0.01)\n",
    "\n",
    "            # Increment the frame index\n",
    "            frame_index += 1\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Invalid environment. Supported values are 'window' or 'jupyter'.\")\n",
    "        exit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:29.914136Z",
     "start_time": "2023-11-26T14:37:21.634932Z"
    },
    "collapsed": false,
    "id": "T3slfX5pHRBQ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Specify the path to your video file and the desired environment ('window' or 'jupyter')\n",
    "video_path = \"example files/traffic.mp4\"\n",
    "# preview_video(0, env='window')\n",
    "preview_video(video_path, env='window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:29.920178Z",
     "start_time": "2023-11-26T14:37:29.915006Z"
    },
    "collapsed": false,
    "id": "EQ2K9VLlHRBQ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "LVVdvPHLHRBQ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Object detection\n",
    "In the following demonstration, we create a powerful function designed to detect dynamic changes within images. The methodology involves subtracting frames containing objects, such as cars, from a background frame, revealing the distinct differences that represent the objects of interest.\n",
    "\n",
    "This approach is particularly useful for identifying moving objects in a video stream, providing a foundation for tasks like object tracking or anomaly detection. While the function can be applied iteratively across frames for real-time applications, this implementation example focuses on a single iteration for clarity and ease of understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9_Ku0BUHRBR"
   },
   "source": [
    "### Define specific frames\n",
    "\n",
    "In the first step, we select two frames from which one will represent our background and the other the image we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:45:24.551391Z",
     "start_time": "2023-11-26T14:45:24.307960Z"
    },
    "collapsed": false,
    "id": "qDXrJUstHRBR",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"example files/traffic.mp4\")\n",
    "\n",
    "# Set the position of the video capture to frame 119 (change the frame number accordingly)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 119)\n",
    "\n",
    "# Read the frame at position 119, which will be used as the background image\n",
    "_, background_image = cap.read()\n",
    "\n",
    "# Set the position of the video capture to frame 23 (change the frame number accordingly)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 23)\n",
    "\n",
    "# Read the frame at position 23, which will be used as the test frame with cars\n",
    "_, image_with_cars = cap.read()\n",
    "\n",
    "# Display the background and test frames side by side using matplotlib\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plotting the background image\n",
    "plt.subplot(121)\n",
    "plt.title(\"Background\")\n",
    "plt.imshow(background_image)\n",
    "\n",
    "# Plotting the test frame with cars\n",
    "plt.subplot(122)\n",
    "plt.title(\"Test frame\")\n",
    "plt.imshow(image_with_cars)\n",
    "\n",
    "# Show the matplotlib plot\n",
    "plt.show()\n",
    "\n",
    "# Close the video capture object\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6plHb7AGHRBR",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Frame subtraction\n",
    "In this process, we calculate the difference between the current frame and the background. Taking the absolute value of this difference allows us to accentuate variations and unveil the distinct features present in the scene.\n",
    "\n",
    "To filter out inconsequential deviations, we employ a relatively high threshold. This choice is informed by the dynamic nature of the background, which undergoes subtle changes in exposure throughout the recording. By setting a discerning threshold, we ensure that only significant differences are considered, enhancing the accuracy and relevance of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:45:25.094688Z",
     "start_time": "2023-11-26T14:45:24.908849Z"
    },
    "collapsed": false,
    "id": "_PUozHJwHRBR",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the absolute difference between grayscale representations of background and image with cars\n",
    "diff_image = np.abs(rgb2gray(background_image) - rgb2gray(image_with_cars))\n",
    "\n",
    "# Threshold the difference image to create a binary mask\n",
    "# Values greater than 50 are set to 255, representing significant differences\n",
    "diff_thresholded = diff_image * 255 > 50\n",
    "\n",
    "# Display the results using Matplotlib\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot the thresholded difference image with a title\n",
    "plt.subplot(121)\n",
    "plt.title(\"Isolated Individual Cars\")\n",
    "plt.imshow(diff_image ** 2)\n",
    "\n",
    "# Plot the squared difference image with a title\n",
    "plt.subplot(122)\n",
    "plt.title(\"Thresholded Difference with Background\")\n",
    "plt.imshow(diff_thresholded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bT35X0MFHRBS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Morphological cleaning of mask\n",
    "After subtracting the background, vehicles and their shadows become noticeable. To enhance accuracy, we apply morphological operations to remove noise, fill vehicle contours, and ensure each vehicle is represented as a single connected object. Closing operations fill contours, while opening operations remove small fragments associated with a single vehicle, reducing false positives. This process refines vehicle detection, ensuring our algorithm accurately identifies and isolates vehicles in the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:45:26.306745Z",
     "start_time": "2023-11-26T14:45:26.097385Z"
    },
    "collapsed": false,
    "id": "0Z_6k181HRBS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply morphological opening to remove small fragments and holes\n",
    "diff_thresholded_processed =  morphology.closing(morphology.closing(morphology.closing(morphology.remove_small_holes(diff_thresholded, area_threshold=1000))))\n",
    "\n",
    "# Display the images before and after morphological operations\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot the original thresholded difference image\n",
    "plt.subplot(131)\n",
    "plt.title(\"Mask before\")\n",
    "plt.imshow(diff_thresholded)\n",
    "\n",
    "# Plot the image after morphological opening\n",
    "plt.subplot(132)\n",
    "plt.title(\"Mask after\")\n",
    "plt.imshow(diff_thresholded_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UJjyF6qhHRBS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Specifying mask areas\n",
    "The label function, often found in image processing libraries like `scikit-image`, is used to identify and label connected components (regions with the same pixel value) in a binary or grayscale image. The primary purpose is to assign unique labels to distinct objects or regions within the image, facilitating subsequent analysis or measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:46:24.004053Z",
     "start_time": "2023-11-26T14:46:23.886296Z"
    },
    "collapsed": false,
    "id": "xpaGEiPBHRBS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Label connected components in the processed thresholded difference image\n",
    "label_img = label(diff_thresholded_processed, connectivity=1.5)\n",
    "\n",
    "# Create a binary mask for objects with ID 6 (for example), indexes are assigned randomly\n",
    "selected_objects = (label_img == 6)\n",
    "\n",
    "# Display the original thresholded difference image\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot the labeled image with all objects\n",
    "plt.subplot(132)\n",
    "plt.title(\"Separated cars\")\n",
    "plt.imshow(label_img, cmap='viridis')\n",
    "\n",
    "# Plot the labeled image with only objects of ID 1\n",
    "plt.subplot(133)\n",
    "plt.title(\"Solo object\")\n",
    "plt.imshow(selected_objects, cmap='gray')\n",
    "\n",
    "# Show the matplotlib plot\n",
    "plt.show()\n",
    "\n",
    "# Print all indexes\n",
    "print(np.unique(label_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7uFZjRUvHRBS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Detect and visualise centroids of cars\n",
    "This code is designed to visualize and highlight the centroids of labeled regions (objects) in an image that contains cars. The code uses the `regionprops` function to obtain properties of labeled regions, and it iterates over these regions, filtering out those with an area (number of pixels) less than 25. For each qualifying region, a red circle is added to the plot to visually represent its centroid. This can be a helpful step in object detection and analysis, where the goal is to identify and highlight significant features, such as car centroids, within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:30.738864Z",
     "start_time": "2023-11-26T14:37:30.658575Z"
    },
    "collapsed": false,
    "id": "mleaDskIHRBS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up the figure and axis for plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Highlighted car centroids\")  # Set the title of the plot\n",
    "ax.imshow(image_with_cars)  # Display the original image with cars\n",
    "\n",
    "threshold_area_size = 20\n",
    "# Get region properties for labeled objects in the image\n",
    "regions = skimage.measure.regionprops(np.squeeze(label_img))\n",
    "\n",
    "filtered_regions = [region for region in regions if region.area >= threshold_area_size]\n",
    "print(f\"Number of cars: {len(filtered_regions)}\")\n",
    "# Iterate over each labeled region in the image\n",
    "for index, region in enumerate(regions):\n",
    "    # Filter regions based on area (number of pixels)\n",
    "    if region.area >= threshold_area_size:\n",
    "        # Print the area of the filtered region (for debugging or verification)\n",
    "        print(f'Area index {index} | Area pixel size {region.area}')\n",
    "\n",
    "        # Create a visualization of the centroid using a circle patch\n",
    "        centroid_viz = patches.Circle((region.centroid[1], region.centroid[0]), radius=2, edgecolor='white', facecolor='red')\n",
    "\n",
    "        # Add the centroid visualization to the plot\n",
    "        ax.add_patch(centroid_viz)\n",
    "\n",
    "# Show the matplotlib plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "PypNc86GHRBT",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Implementing the algorithm on the whole video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:41.548879Z",
     "start_time": "2023-11-26T14:37:30.741252Z"
    },
    "collapsed": false,
    "id": "cSU5Ltt-HRBT",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "threshold_area_size = (50, 5000) # Create up and down threshold\n",
    "cap = cv2.VideoCapture(\"example files/traffic.mp4\")\n",
    "\n",
    "# Set the position of the video capture to frame 119 (change the frame number accordingly)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 119)\n",
    "\n",
    "# Read the frame at position 119, which will be used as the background image\n",
    "_, background_image = cap.read()\n",
    "background_image = rgb2gray(background_image)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "# Create a larger window\n",
    "cv2.namedWindow('Filtered Regions', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Filtered Regions', 800, 600)  # Adjust the dimensions as needed\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Calculate the absolute difference between grayscale representations of background and image with cars\n",
    "    diff_image = np.abs( background_image - rgb2gray(frame))\n",
    "\n",
    "    # Threshold the difference image to create a binary mask\n",
    "    # Values greater than 50 are set to 255, representing significant differences\n",
    "    diff_thresholded = diff_image * 255 > 30\n",
    "\n",
    "    # Apply morphological opening to remove small fragments and holes\n",
    "    # diff_thresholded_processed = morphology.opening(morphology.remove_small_holes(diff_thresholded, area_threshold=64))\n",
    "    diff_thresholded_processed =  morphology.closing(morphology.closing(morphology.closing(morphology.remove_small_holes(diff_thresholded, area_threshold=1000))))\n",
    "\n",
    "    # Label connected components in the processed thresholded difference image\n",
    "    label_img = label(diff_thresholded_processed, connectivity=1.5)\n",
    "\n",
    "    # Get region properties for labeled objects in the image\n",
    "    regions = skimage.measure.regionprops(np.squeeze(label_img))\n",
    "\n",
    "    filtered_regions = [region for region in regions if threshold_area_size[0] <= region.area < threshold_area_size[1]]\n",
    "\n",
    "    # Draw bounding boxes around filtered regions\n",
    "    for region in filtered_regions:\n",
    "        # get bBox indexes\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        # get centroid indexes\n",
    "        local_centroid = tuple([int(region.centroid[1]),int(region.centroid[0])])\n",
    "        cv2.rectangle(frame, (minc, minr), (maxc, maxr), (0, 255, 0), 2)\n",
    "        cv2.circle(frame, local_centroid , 1, (255, 0, 0), 2)\n",
    "        # Add text\n",
    "        frame = add_text_to_frame(frame, \"CARs NUMBER: {}\".format(len(filtered_regions)), position=(5,10), color=(0,0,255))\n",
    "\n",
    "\n",
    "    cv2.resizeWindow('Filtered Regions', 800, 600)  # Adjust the dimensions as needed\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Filtered Regions', frame)\n",
    "    # if len(filtered_regions) >= 4:\n",
    "    #     break\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "    time.sleep(.05)\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wPR6G4EkHRBT",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Additional task\n",
    "Improve car detection on the video by:\n",
    "\n",
    "* applying blur on frames to reduce noise from the camera\n",
    "* testing various parameter values to try to find their optimal setting (e.g. `threshold_area_size`, segmentation threshold, morphological operation parameters, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T14:37:52.289610Z",
     "start_time": "2023-11-26T14:37:41.549303Z"
    },
    "collapsed": false,
    "id": "Aro_yBcvHRBT",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
