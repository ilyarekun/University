{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KKUI CIT Course - Neural networks - Week_03 - Backpropagation\n",
    "(This is the code for prove of explanation in [pdf file](materials/Backpropagation.pdf))\n",
    "\n",
    "\n",
    "## What is chain rule?\n",
    "The chain rule is a fundamental concept in calculus that describes how to find the derivative of a composite function. It states that if you have a function composed of two or more functions, you can find its derivative by taking the derivative of the outer function with respect to its input, and then multiplying it by the derivative of the inner function with respect to its input, forming a chain of derivatives.\n",
    "\n",
    "## What is a backpropagation\n",
    "Backpropagation is a fundamental algorithm used in training artificial neural networks, particularly in the context of gradient-based optimization methods. It is a technique for efficiently computing the gradients of the loss function with respect to the parameters of the neural network, which are then used to update the parameters in the direction that minimizes the loss.\n",
    "\n",
    "Here's a high-level overview of how backpropagation works:\n",
    "\n",
    "**1. Forward Pass:** During the forward pass, input data is fed into the neural network, and its output is computed layer by layer through various activation functions until the final output is generated. This process involves multiplying the input data by the weights of each layer, applying activation functions, and passing the results to the next layer.\n",
    "**2. Loss Calculation:** Once the output is computed, it is compared to the true labels or targets, and a loss function is calculated to quantify the difference between the predicted output and the actual target.\n",
    "****3. Backward Pass (Backpropagation):** In this step, the gradients of the loss function with respect to each parameter of the network are computed using the chain rule of calculus. The process starts from the output layer and moves backward through the network, computing the gradients layer by layer. This is where the name \"backpropagation\" comes from, as the gradients are propagated backward through the network.\n",
    "4. Gradient Descent:** Finally, the computed gradients are used to update the parameters of the network in the direction that minimizes the loss function. This is typically done using optimization algorithms like stochastic gradient descent (SGD), Adam, or RMSprop.\n",
    "\n",
    "By iteratively performing forward passes to compute predictions, backward passes to compute gradients, and parameter updates using gradient descent, the neural network learns to improve its predictions over time by adjusting its parameters to minimize the loss function.\n",
    "\n",
    "Backpropagation is a foundational concept in training neural networks and has enabled the development of deep learning models that achieve state-of-the-art performance across various tasks, including image recognition, natural language processing, and reinforcement learning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<PowBackward0>)\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# This is the parameter we want to optimize -> requires_grad=True\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# forward pass to compute loss\n",
    "y_predicted = w * x\n",
    "loss = (y_predicted - y)**2\n",
    "print(loss)\n",
    "\n",
    "# backward pass to compute gradient dLoss/dw\n",
    "loss.backward()\n",
    "print(w.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T10:56:55.808585Z",
     "start_time": "2024-02-26T10:56:55.804855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "# # update weights\n",
    "# # next forward and backward pass...\n",
    "#\n",
    "# # continue optimizing:\n",
    "# # update weights, this operation should not be part of the computational graph\n",
    "# with torch.no_grad():\n",
    "#     w -= 0.01 * w.grad\n",
    "# # don't forget to zero the gradients\n",
    "# w.grad.zero_()\n",
    "\n",
    "# next forward and backward pass..."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
