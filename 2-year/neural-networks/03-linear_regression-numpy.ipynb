{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KKUI CIT Course - Neural networks - Week_02 - Linear regression (low-level - Gabriela)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506905\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Compute every step manually\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x\n",
    "\n",
    "# here : f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "# X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "# Y = np.array([2, 4, 6, 7], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.mean(2*x*(y_pred - y))\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 2\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:47:04.063943Z",
     "start_time": "2024-02-18T20:47:04.057137Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "1.8666667208672416"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:37:38.359793Z",
     "start_time": "2024-02-18T20:37:38.350414Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# Generate some test data points to evaluate the trained model\n",
    "test_x = np.array([[5], [5.3], [6.20]])\n",
    "\n",
    "\n",
    "# Get the model predictions for the test data\n",
    "test_y = forward(test_x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:37:38.661777Z",
     "start_time": "2024-02-18T20:37:38.654686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the original data points (red circles), the predicted regression line (blue line), and additional testing values (green crosses)\n",
    "plt.plot(X, Y, 'ro', label='Original data')  # Plot original data points\n",
    "plt.plot(X, y_pred, 'b', label='Fitted line')    # Plot fitted regression line\n",
    "\n",
    "# Add the testing data points along with their predicted values (marked with green crosses)\n",
    "plt.plot(test_x, test_y, 'gx', label='Testing data')  # Plot testing data points\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Display legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:37:38.898051Z",
     "start_time": "2024-02-18T20:37:38.814979Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement auto gradient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Here we replace the manually computed gradient with autograd\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x\n",
    "\n",
    "# here : f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:45:20.646328Z",
     "start_time": "2024-02-18T20:45:20.628562Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add PyTorch loss and optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x\n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:50:31.878925Z",
     "start_time": "2024-02-18T20:50:31.810759Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add PyTorch model for linear regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = 3.273\n",
      "epoch  1 : w =  0.7830569744110107  loss =  tensor(12.7148, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.6041162014007568  loss =  tensor(0.4103, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.7420947551727295  loss =  tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.7700257301330566  loss =  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.7800861597061157  loss =  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.7871077060699463  loss =  tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.7934808731079102  loss =  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.7995948791503906  loss =  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.8055168390274048  loss =  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.8112618923187256  loss =  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "epoch  101 : w =  1.816837191581726  loss =  tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
      "epoch  111 : w =  1.8222476243972778  loss =  tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
      "epoch  121 : w =  1.8274980783462524  loss =  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "epoch  131 : w =  1.8325936794281006  loss =  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "epoch  141 : w =  1.8375385999679565  loss =  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "epoch  151 : w =  1.8423376083374023  loss =  tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "epoch  161 : w =  1.8469947576522827  loss =  tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "epoch  171 : w =  1.8515143394470215  loss =  tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
      "epoch  181 : w =  1.8559006452560425  loss =  tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "epoch  191 : w =  1.8601571321487427  loss =  tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "epoch  201 : w =  1.8642878532409668  loss =  tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
      "epoch  211 : w =  1.86829674243927  loss =  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
      "epoch  221 : w =  1.8721870183944702  loss =  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
      "epoch  231 : w =  1.8759623765945435  loss =  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "epoch  241 : w =  1.8796262741088867  loss =  tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
      "epoch  251 : w =  1.8831820487976074  loss =  tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "epoch  261 : w =  1.8866328001022339  loss =  tensor(0.0187, grad_fn=<MseLossBackward0>)\n",
      "epoch  271 : w =  1.8899815082550049  loss =  tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "epoch  281 : w =  1.8932313919067383  loss =  tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "epoch  291 : w =  1.8963851928710938  loss =  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "epoch  301 : w =  1.8994457721710205  loss =  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "epoch  311 : w =  1.9024159908294678  loss =  tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
      "epoch  321 : w =  1.9052985906600952  loss =  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "epoch  331 : w =  1.9080958366394043  loss =  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "epoch  341 : w =  1.9108105897903442  loss =  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "epoch  351 : w =  1.9134451150894165  loss =  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "epoch  361 : w =  1.916001796722412  loss =  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "epoch  371 : w =  1.918483018875122  loss =  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "epoch  381 : w =  1.9208910465240479  loss =  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "epoch  391 : w =  1.9232277870178223  loss =  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "epoch  401 : w =  1.9254955053329468  loss =  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "epoch  411 : w =  1.9276962280273438  loss =  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "epoch  421 : w =  1.9298322200775146  loss =  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "epoch  431 : w =  1.9319047927856445  loss =  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "epoch  441 : w =  1.9339163303375244  loss =  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "epoch  451 : w =  1.935868263244629  loss =  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "epoch  461 : w =  1.93776273727417  loss =  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "epoch  471 : w =  1.9396010637283325  loss =  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "epoch  481 : w =  1.9413851499557495  loss =  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "epoch  491 : w =  1.9431166648864746  loss =  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "epoch  501 : w =  1.9447969198226929  loss =  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "epoch  511 : w =  1.946427583694458  loss =  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "epoch  521 : w =  1.9480100870132446  loss =  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "epoch  531 : w =  1.9495458602905273  loss =  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "epoch  541 : w =  1.9510360956192017  loss =  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "epoch  551 : w =  1.9524824619293213  loss =  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "epoch  561 : w =  1.9538861513137817  loss =  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "epoch  571 : w =  1.955248236656189  loss =  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "epoch  581 : w =  1.9565701484680176  loss =  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "epoch  591 : w =  1.9578529596328735  loss =  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "epoch  601 : w =  1.9590978622436523  loss =  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "epoch  611 : w =  1.9603060483932495  loss =  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "epoch  621 : w =  1.961478590965271  loss =  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "epoch  631 : w =  1.9626166820526123  loss =  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "epoch  641 : w =  1.9637209177017212  loss =  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "epoch  651 : w =  1.9647924900054932  loss =  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "epoch  661 : w =  1.9658324718475342  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "epoch  671 : w =  1.966841697692871  loss =  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "epoch  681 : w =  1.9678211212158203  loss =  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "epoch  691 : w =  1.9687716960906982  loss =  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "epoch  701 : w =  1.9696941375732422  loss =  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "epoch  711 : w =  1.970589280128479  loss =  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "epoch  721 : w =  1.971458077430725  loss =  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "epoch  731 : w =  1.9723011255264282  loss =  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "epoch  741 : w =  1.9731192588806152  loss =  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "epoch  751 : w =  1.9739131927490234  loss =  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "epoch  761 : w =  1.9746840000152588  loss =  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "epoch  771 : w =  1.9754317998886108  loss =  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "epoch  781 : w =  1.9761574268341064  loss =  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "epoch  791 : w =  1.9768617153167725  loss =  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "epoch  801 : w =  1.9775452613830566  loss =  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "epoch  811 : w =  1.9782085418701172  loss =  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "epoch  821 : w =  1.9788522720336914  loss =  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "epoch  831 : w =  1.9794769287109375  loss =  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "epoch  841 : w =  1.9800831079483032  loss =  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "epoch  851 : w =  1.9806714057922363  loss =  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "epoch  861 : w =  1.981242299079895  loss =  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "epoch  871 : w =  1.9817965030670166  loss =  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "epoch  881 : w =  1.9823341369628906  loss =  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "epoch  891 : w =  1.982856035232544  loss =  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "epoch  901 : w =  1.9833623170852661  loss =  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "epoch  911 : w =  1.983853816986084  loss =  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "epoch  921 : w =  1.9843307733535767  loss =  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "epoch  931 : w =  1.9847935438156128  loss =  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "epoch  941 : w =  1.9852428436279297  loss =  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "epoch  951 : w =  1.9856786727905273  loss =  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "epoch  961 : w =  1.9861016273498535  loss =  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "epoch  971 : w =  1.986512303352356  loss =  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "epoch  981 : w =  1.9869108200073242  loss =  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "epoch  991 : w =  1.987297534942627  loss =  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 9.975\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x\n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "'''\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T20:51:30.564097Z",
     "start_time": "2024-02-18T20:51:30.393283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
