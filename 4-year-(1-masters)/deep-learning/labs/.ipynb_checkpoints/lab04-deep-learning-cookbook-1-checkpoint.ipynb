{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Networks - Optimization and Tips\n",
    "\n",
    "In the previous exercise, we implemented a simple convolutional network, and in solving your assignments, you will encounter other commonly used topologies and architectures. Defining a neural network is just the first (and simplest) step in developing models that you will later use in real-world applications. To effectively use your models, you need to train them properly, but this process is complicated by several factors. The goal of today's exercise is to familiarize you with these problems and the basic ways to address them.\n",
    "\n",
    "Specifically, we will look at the hyperparameters of (deep) neural networks, how to select their values appropriately, and methods to accelerate the training of deep neural networks, which you will almost always need, as computational resources for training networks are limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Heuristic for Designing Deep Networks\n",
    "\n",
    "| Task                    | Number of Inputs | Number of Outputs | Activation Function in Output Layer |\n",
    "|-------------------------|------------------|-------------------|-------------------------------------|\n",
    "| Binary Classification    | Number of Features | 2                 | Sigmoid                            |\n",
    "| Multiclass Classification| Number of Features | C (number of classes) | Softmax                            |\n",
    "| Prediction              | Number of Features | Usually 1         | ReLU/Linear                        |\n",
    "\n",
    "When designing deep networks, it is beneficial to use many layers only in convolutional networks. In the fully connected part of the network, it is recommended to use a maximum of 3 hidden layers (adding more hidden layers does not significantly increase accuracy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training, Testing, and Validation Sets\n",
    "\n",
    "The success, or failure, of trained models depends primarily not only on their topology and hyperparameters but also on the data used and its distribution. The most important prerequisite for the successful deployment of neural networks is that **the examples in the training data match the expected real-world input examples** when using the model for real prediction. Misalignment can cause problems, for instance, when training on high-resolution images and using the model for prediction on low-resolution images, and so on.\n",
    "\n",
    "In the context of training and testing neural networks, data is usually divided into three parts: **training** (*training*), **validation** (*development* or *validation*), and **test** (*test*) sets. Training data is used to adjust the network's parameters so that the network's predictions are as accurate as possible (training phase). The validation set is used to evaluate the network's accuracy during training, especially to help decide whether it makes sense to continue training the model. Finally, the test set is used to evaluate the model from the perspective of real-world use — our goal is to simulate the most suitable use cases.\n",
    "\n",
    "In literature, you will often encounter cases where only two sets are used, labeled as *training* and *test* sets. While this naming is common, it should be noted that the *test* set in this context almost always plays the role of the *validation* set.\n",
    "\n",
    "A basic recommendation for dividing available data for training neural networks is *70/30%*, or *60/20/20%* when splitting into both validation and test sets. This division serves as a good heuristic for smaller datasets (tens of thousands of examples), but if you have large data available for training, assigning *20-30%* of examples to the validation or test set is often unnecessary.\n",
    "\n",
    "In such cases, you need to consider the initial purpose of using these sets — namely, evaluating model accuracy, comparing two models, and simulating use cases. Therefore, if you have hundreds of thousands or millions of data points available, you can often assign up to *90-98%* of the data to the training set, and then split the remaining part between the validation and test sets (usually evenly, or more towards validation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluating a Neural Network\n",
    "\n",
    "The way a trained model is evaluated depends, of course, on the specific use case. In most cases, however, your task is to achieve the highest possible accuracy of the model and the smallest possible error. In other cases, such as when the goal is to replace an existing process, it is sufficient if your solution outperforms existing approaches in certain metrics.\n",
    "\n",
    "For predictive tasks, the primary metric is the error value that you want to minimize. In classification, the behavior of the network can be illustrated using a confusion matrix or contingency table. A confusion matrix is a tabular representation where the rows represent the expected classes and the columns represent the computed (predicted) classes. The cells of the table contain the counts of examples classified in a given combination of expected and predicted classes. An ideal classifier will have all values along the main diagonal (more information can be found on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)).\n",
    "\n",
    "From the confusion matrix, we can calculate additional metrics such as accuracy, recall, and precision.\n",
    "\n",
    "Accuracy describes the classifier itself and is calculated as follows:\n",
    "\n",
    "$ACC = \\frac{TP + TN}{P + N}$\n",
    "\n",
    "where $TP + TN$ is the sum of correctly classified examples (on the main diagonal), and $P + N$ is the total number of examples.\n",
    "\n",
    "Recall (sensitivity) and precision describe the classifier for a given class and are calculated as follows:\n",
    "\n",
    "$REC = \\frac{TP}{P}$\n",
    "\n",
    "$PREC = \\frac{TP}{TP + FP}$\n",
    "\n",
    "where $TP$ is the number of correctly classified examples from the given class, $P$ is the number of examples from that class in the test set, and $FP$ is the number of examples from the test set incorrectly classified into this class.\n",
    "\n",
    "Often, the F1 score is also calculated, which is the harmonic mean of recall and precision:\n",
    "\n",
    "$F1 = 2 \\cdot \\frac{REC \\cdot PREC}{REC + PREC}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. *Bias* and *Variance*\n",
    "\n",
    "The development of deep models is an iterative process, where you design a certain architecture, evaluate it, and adjust it based on the evaluation conclusions to meet the requirements. In this regard, two indicators help: *bias* and *variance* of the network. *Bias* reflects the ability of the network to learn to make accurate predictions, while *variance* reflects its ability to generalize, i.e., to generalize predictions based on the training data. These two indicators are obtained from numerical metrics - training set error (TSE) and development set error (DSE). These errors can be expressed as mean error in regression or percentage accuracy in classification. The training error tells us something about the *bias*, while comparing the training and development errors helps determine the *variance*. High *bias* is the result of underfitting, while high *variance* is caused by overfitting.\n",
    "\n",
    "It is important to note that when developing neural networks, you should determine a target accuracy and also consider human-level accuracy for the task at hand, or the accuracy of a random classifier. During development, you will most often encounter the following four cases:\n",
    "\n",
    "1. $TSE < DSE$, $TSE$ close to or lower than human level - your model is overfitting and cannot generalize its knowledge well for predictions; you should stop training.\n",
    "2. $TSE \\approx DSE$; not close to human level - high *bias*, your model is unable to train well on the training data; you need more data or better data selection, or you should preprocess the data.\n",
    "3. $TSE >>, DSE >>$ - high *bias* and *variance*; your model is overfitting on certain parts of the data.\n",
    "4. $TSE <<, DSE <<$ - low *bias* and *variance*; ideal case.\n",
    "\n",
    "In general, high *bias* is addressed by using a more robust topology, longer training, using different optimization algorithms, or altering the network architecture. High *variance* is addressed by obtaining a larger dataset, regularizing the data, and using a different architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Andrew Ng: Improving Deep Neural Networks - Hyperparameter Tuning](https://www.youtube.com/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
