{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KKUI CIT Course - Neural networks - Week_02 - Autograd PyTorch\n",
    "\n",
    "## A Gentle Introduction ``to torch.autograd``\n",
    "[A Gentle Introduction to (PyTorch website)](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "\n",
    "``torch.autograd`` is PyTorchâ€™s automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train.\n",
    "\n",
    "Background\n",
    "\n",
    "Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "**Forward Propagation:** In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "**Backward Propagation:** In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. For a more detailed walkthrough of backprop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "import torch\n",
    "# The autograd package provides automatic differentiation\n",
    "# for all operations on Tensors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T17:25:42.307077Z",
     "start_time": "2024-02-18T17:25:42.304249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# requires_grad = True -> tracks all operations on the tensor.\n",
    "x = torch.randn(3, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T17:25:42.563770Z",
     "start_time": "2024-02-18T17:25:42.483052Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x -> tensor([-0.8917, -0.6851,  1.7303], requires_grad=True)\n",
      "y -> tensor([1.1083, 1.3149, 3.7303], grad_fn=<AddBackward0>)\n",
      "z -> tensor([ 3.6850,  5.1870, 41.7449], grad_fn=<MulBackward0>)\n",
      "z sum -> 16.872297286987305\n",
      "x grad -> tensor([2.2166, 2.6298, 7.4606])\n"
     ]
    }
   ],
   "source": [
    "# Enable gradient tracking for tensor x\n",
    "# x.requires_grad_(True)\n",
    "\n",
    "# Define a new tensor y as a result of an operation on x\n",
    "y = x + 2\n",
    "\n",
    "# Since y is created as a result of an operation, it has a grad_fn attribute\n",
    "# grad_fn references a Function that has created the tensor\n",
    "print(f\"x -> {x}\")  # created by the user -> grad_fn is None\n",
    "print(f\"y -> {y}\")\n",
    "\n",
    "# Perform more operations on y\n",
    "z = y * y * 3\n",
    "print(f\"z -> {z}\")\n",
    "\n",
    "# Calculate the mean of z\n",
    "z = z.mean()\n",
    "print(f\"z sum -> {z}\")\n",
    "\n",
    "# Perform backpropagation to compute the gradients\n",
    "# When computation finishes, call .backward() to compute gradients automatically\n",
    "# The gradient for this tensor will be accumulated into .grad attribute\n",
    "# It represents the partial derivative of the function with respect to the tensor\n",
    "z.backward()\n",
    "print(f\"x grad -> {x.grad}\")\n",
    "\n",
    "# Detach tensor x from the computational graph to demonstrate example of accumulating gradient\n",
    "# x = x.detach()\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivatives while applying the chain rule\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T17:25:42.701851Z",
     "start_time": "2024-02-18T17:25:42.682445Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  645.5250, -1758.2660,  1596.3779], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor 'x' of shape (3,) with requires_grad set to True to enable gradient tracking\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Perform a series of operations on 'x' to obtain tensor 'y'\n",
    "# 'y' will have the same shape as 'x' and will contain the result of the operations\n",
    "y = x * 2  # Multiply 'x' by 2\n",
    "for _ in range(10):\n",
    "    y = y * 2  # Multiply 'y' by 2 repeatedly, resulting in a non-scalar output\n",
    "\n",
    "# Print the tensor 'y' and its shape\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "# Create a tensor 'v' representing the gradient with respect to 'y'\n",
    "# This tensor specifies how much each element of 'y' contributes to the final gradient\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "\n",
    "# Use backward() with an additional 'gradient' argument to compute gradients of non-scalar outputs\n",
    "# 'v' acts as the gradient argument here, indicating the gradients that should be backpropagated\n",
    "y.backward(v)\n",
    "\n",
    "# Print the gradients of 'x' after backpropagation\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T17:25:42.833512Z",
     "start_time": "2024-02-18T17:25:42.826769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x11e9aae80>\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if requires_grad is enabled for tensor 'a'\n",
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)\n",
    "\n",
    "# Perform operations on tensor 'a' to create tensor 'b'\n",
    "b = ((a * 3) / (a - 1))\n",
    "\n",
    "# Print the gradient function of tensor 'b'\n",
    "# Since 'b' is a result of operations on 'a', it will have a gradient function\n",
    "print(b.grad_fn)\n",
    "\n",
    "# Enable requires_grad for tensor 'a' using requires_grad_()\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "# Perform operations on tensor 'a' to create tensor 'b'\n",
    "b = (a * a).sum()\n",
    "\n",
    "# Print the gradient function of tensor 'b'\n",
    "# Since 'b' is a result of operations on 'a', it will have a gradient function\n",
    "print(b.grad_fn)\n",
    "\n",
    "# Detach tensor 'a' to create tensor 'b' without gradient computation\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "b = a.detach()\n",
    "print(b.requires_grad)\n",
    "\n",
    "# Wrap the operation in 'with torch.no_grad()' to temporarily disable gradient tracking\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((a ** 2).requires_grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T17:25:43.065183Z",
     "start_time": "2024-02-18T17:25:43.049615Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "tensor(4.8000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Final example\n",
    "# Create a tensor 'weights' with requires_grad enabled for optimization\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# Run training for multiple epochs\n",
    "for epoch in range(3):\n",
    "    # Just a dummy example: compute model output (sum of weights multiplied by 3)\n",
    "    model_output = (weights * 3).sum()\n",
    "\n",
    "    # Perform backpropagation to compute gradients\n",
    "    model_output.backward()\n",
    "\n",
    "    # Print the gradients of weights\n",
    "    print(weights.grad)\n",
    "\n",
    "    # Optimize the model by adjusting weights using gradient descent\n",
    "    # Update weights using gradient descent formula: new_weights = old_weights - learning_rate * gradient\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # Important step: Empty the gradients before the next optimization step to avoid accumulation\n",
    "    weights.grad.zero_()\n",
    "\n",
    "# After training, print the final weights and model output\n",
    "print(weights)\n",
    "print(model_output)\n",
    "\n",
    "# Note: Optimizers provided by torch.optim automatically handle gradient updates and zeroing gradients\n",
    "# Example usage:\n",
    "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training loop:\n",
    "# optimizer.step()  # Update weights based on gradients\n",
    "# optimizer.zero_grad()  # Clear gradients for the next iteration\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T17:25:43.568051Z",
     "start_time": "2024-02-18T17:25:43.549851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
